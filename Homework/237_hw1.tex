\documentclass[a4paper, 11pt]{article}
\usepackage[margin=.5in]{geometry}
\usepackage{graphicx,amssymb,amstext,amsmath,amsthm,enumerate,float,textcomp}
\usepackage{pdfpages}
%\usepackage{listings}
\makeatletter
\renewcommand\@makefntext[1]{\leftskip=1em\hskip-2em\@makefnmark#1}
\makeatother

\begin{document}
\flushright
\begin{center}
\Large
STA 237  --- Time Series Analysis\\ Homework Series 1 \\ Rex Cheung  $\&$ Eliot Paisley \\ April 10, 2014 \\
\large  Prof. A.Aue \\ UC Davis
\end{center}

\flushleft 
\hrule \hrule 
\vspace{0.2in}
\begin{itemize}

	\item \textbf{Problem 1: [Stationarity]} Let $(Z_t: \; t \in \mathbb{Z})$ be a sequence of independent zero mean and normal random variables with variance $\sigma^2$ and let $a, b$, and $c$ be constants. Which of the following processess are weakly and/or strictly stationary? For wach weakly stationary process specify the mean and ACVF. 
	
	
	\begin{enumerate}[(a)]
		\item $X_t = a + bZ_t + cZ_{t-1}$. \newline 
		
			\underline{\emph{Answer:}} Let $i,j \in \mathbb{Z}$ be any two integers. Then, 
			$$E[X_i] = E[a + bZ_i + cZ_{i-1}] = a = E[a + bZ_j + cZ_{j-1}] = E[X_j],$$
			and
			$$Var[X_i] = Var[a + bZ_i + cZ_{i-1}] = 0 + b^2\sigma^2 + c^2\sigma^2 = Var[a + bZ_j + cZ_{j-1}] = Var[X_j], $$
where we have implicitly used the fact that every pair of $Z_t$ have zero covariance. \newline 

Thus, with knowledge that linear combinations of normally distributed random variables are normal, we have 
$$X_i,X_j \sim \mathcal{N}(a,(b^2+c^2)\sigma^2) .$$
Since $i,j$ were arbitrary, we have that every element $X_t$ has the same distribution, which implies that $X_t$ is \emph{strictly stationary}. \newline 

Since we have explicitly shown that we have finite variance, then we may also conclude that $X_t$ is \emph{weakly stationary}. $E[X_t] = a \; \forall t$, and note that 
$$Cov(X_{t+1},X_t) = Cov(a + bZ_{t+1} + cZ_{t}, a + bZ_t + cZ_{t-1} ) = Cov(cZ_t,bZ_t) = bc\sigma^2.  $$
Thus, 
$$\gamma(h) = \left\{\begin{array}{cc} (b^2+c^2)\sigma^2 & h = 0 \\ bc\sigma^2 & h=\pm 1 \\ 0 & |h|>1 \end{array}\right. .$$

		\item $X_t = Z_t\cos(ct) + Z_{t-1}\sin(ct)$ \newline 
		
			\underline{\emph{Answer:}} Let $i,j \in \mathbb{Z}$ be any two integers. Then, 
			$$E[X_i] = E[Z_i\cos(ci) + Z_{i-1}\sin(ci)] = 0 = E[Z_j\cos(cj) + Z_{j-1}\sin(cj)] = E[X_j],$$
			and
			$$Var[X_i] = Var[Z_i\cos(ci) + Z_{i-1}\sin(ci)] = \sigma^2\cos^2(ci) + \sigma^2\sin^2(ci) = \sigma^2, $$
			$$Var[X_j] = Var[Z_j\cos(cj) + Z_{j-1}\sin(cj)] = \sigma^2\cos^2(cj) + \sigma^2\sin^2(cj) = \sigma^2, $$

where we have implicitly used the fact that every pair of $Z_t$ have zero covariance. Thus, with knowledge that linear combinations of normally distributed random variables are normal, we have 
$$X_i,X_j \sim \mathcal{N}(0,\sigma^2) .$$
Since $i,j$ were arbitrary, we have that every element $X_t$ has the same distribution, which implies that $X_t$ is \emph{strictly stationary}.  \newline 

Since we have explicitly shown that we have finite variance, then we may also conclude that $X_t$ is \emph{weakly stationary}. $E[X_t] = 0 \; \forall t$, and note that 
\begin{align*}
Cov(X_{t+1},X_t) & = Cov(Z_{t+1}\cos(ct+c) + Z_{t}\sin(ct+c),Z_t\cos(ct) + Z_{t-1}\sin(ct) ) \\
& = Cov(cZ_t,bZ_t) = \sin(ct+c)\cos(ct)\sigma^2
\end{align*}
Thus, 
$$\gamma(h) = \left\{\begin{array}{cc} \sigma^2 & h = 0 \\ \sin(ct+c)\cos(ct)\sigma^2 & h= + 1 \\ \cos(ct+c)\sin(ct)\sigma^2 & h= - 1 \\ 0 & |h|>1 \end{array}\right. .$$
 

		\item $X_t = a + bZ_0$ \newline 
		
			\underline{\emph{Answer:}} Let $i,j \in \mathbb{Z}$ be any two integers. Then, 
			$$E[X_i] = E[a + bZ_0] = a = E[a + bZ_0] = E[X_j],$$
			and
			$$Var[X_i] = Var[a + bZ_0] = 0 + b^2\sigma^2 = Var[a + bZ_0] = Var[X_j].$$

Thus, with knowledge that linear combinations of normally distributed random variables are normal, we have 
$$X_i,X_j \sim \mathcal{N}(0,b^2\sigma^2) .$$
Since $i,j$ were arbitrary, we have that every element $X_t$ has the same distribution, which implies that $X_t$ is \emph{strictly stationary}. \newline 

Since we have explicitly shown that we have finite variance, then we may also conclude that $X_t$ is \emph{weakly stationary}. $E[X_t] = 0 \; \forall t$, and 
$$\gamma(h) = b^2\sigma^2, \forall h. $$

		\item $X_t = Z_tZ_{t-1}$ \newline 
		
			\underline{\emph{Answer:}} \\ 

	\end{enumerate}
	
	
	\hrule 
	\newpage 
	\item \textbf{Problem 2: [U.S. Population]} Download the file \texttt{population.xls} from the course website. It contains the size of the population in the U.S.A. at ten-year intervals from 1790 to 2000. 
	
	
	\begin{enumerate}[(a)]
		\item Plot the data. \newline 
		
		\emph{See Figure 1.}
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
	\includegraphics[scale=0.5]{hw1_dataplot.pdf}
  \caption{Plot of the population data for Problem 2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[scale=0.5]{hw1_resid.pdf}
  \caption{Plot of the Residuals against Time for the fitted values in Table 1.}
\end{minipage}
\end{figure}

		\item Assuming the model $X_t = m_t + Z_t$, $E[Z_t] = 0$, fit a polynomial trend $\widehat{m}_t$ to the data. \newline 
		
		\emph{See Table 1 for the fitted values for a $2^{nd}$ degree polynomial}
		
\begin{table}[h!]
\small
\centering
\begin{tabular}{r}
  \hline
	$\widehat{m}_t$\\ 
  \hline
	6110720.45 \\ 
  5637937.68 \\ 
  6501266.64 \\ 
  8700707.34 \\ 
  12236259.78 \\ 
  17107923.95 \\ 
  23315699.86 \\ 
  30859587.50 \\ 
  39739586.88 \\ 
  49955698.00 \\ 
  61507920.85 \\ 
  74396255.44 \\ 
  88620701.76 \\ 
  104181259.82 \\ 
  121077929.62 \\ 
  139310711.15 \\ 
  158879604.42 \\ 
  179784609.42 \\ 
  202025726.16 \\ 
  225602954.63 \\ 
  250516294.84 \\ 
  276765746.79 \\ 
   \hline
\end{tabular}
\caption{Fitted values for a $2^{nd}$ degree polynomial fit}
\end{table}

		\item Plot the residuals $\hat{Z}_t = X_t - \hat{m}_t$. Comment on the quality of the fitted model. \newline 

		\emph{See Figure 2.}
		\item Use the fitted model to predict the population size in 2010 and 2020 (using predicted noise values of zero). \newline 
		
		\emph{See Table 2.}
		
		\begin{table}[h!]
		\centering
		\begin{tabular}{rrr}
			\hline
			& 2010 & 2020 \\ 
			\hline
		$\widehat{m}$ & 304351310.47 & 333272985.89 \\ 
			\hline
		\end{tabular}
		\caption{Predicted population values for the years 2010 and 2020.}
		\end{table}


\end{enumerate}
	

	\hrule 
	\newpage \newpage 
	\item \textbf{Problem 3: [Projection Theorem]} If $\mathcal{M}$ is a closed subspace of a Hilbert Space $\mathcal{H}$ and $x\in\mathcal{H}$, prove that 
	$$\underset{y\in\mathcal{M}}{\min}||x-y|| = \max\left\{|\langle x,z\rangle|: z\in \mathcal{M}^\bot, ||z|| = 1  \right\} ,$$
where $\mathcal{M}^\bot$ is the orthogonal complement of $\mathcal{M}$. \newline 

	\underline{\emph{Answer:}} We show the desired result in two cases:\newline
	
	Case 1: Suppose that $x\in\mathcal{M}$. Then $\underset{y\in\mathcal{M}}{\min}||x-y|| = 0$, where the minimum is obtained at $y = x$. Moreover, $\langle x,z\rangle = 0\; \forall z\in \mathcal{M}^\bot$, due to orthogonality. \newline 
	
	Case 2: Suppose that $x\in\mathcal{M}^\bot$. Then, for $y\in\mathcal{M}, z\in\mathcal{M}^\bot$,  we can write 
	$$\langle x,z \rangle = \langle x-y,z \rangle + \langle y,z\rangle  = \langle x-y,z \rangle,$$
	and by the triangle inequality,
	$$\langle x-y,z \rangle \leq ||x-y|| ||z|| = ||x-y||.$$
	Thus, we have 
	$$\langle x,z \rangle \leq ||x-y||,$$
	and it follows naturally that in order for equality to hold we must have the maximum on the left-hand-side, and the minimum on the right-hand-side. That is, 
	$$\max \langle x,z \rangle  = \min ||x-y||.$$
	
	\hrule 
	\item \textbf{Problem 4: [Prediction Equations]} If $X_t = Z_t - \theta Z_{t-1}$, where $|\theta| <1$ and $(Z_t: t\in\mathbb{Z})$ is a sequence of uncorrelated random variables, each with mean 0 and variance $\sigma^2$, show by checking the prediction equations that the best mean square predictor of $X_{n+1}$ in $\overline{\text{sp}}(X_j: j \leq n)$ is 
	$$\hat{X}_{n+1} = -\sum_{j=1}^\infty \theta^j X_{n+1-j}. $$
	What is the mean squared error of $\hat{X}_{n+1}$?

	\underline{\emph{Answer:}} \newline 
	
	\hrule 


\end{itemize}	
\end{document}












